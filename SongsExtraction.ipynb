{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853a087-3405-465b-8b35-d90b1434c9ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e568953-ac23-4192-a731-7a33b866d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3efc4-4f31-4a60-b451-c0b6f57a8a3d",
   "metadata": {},
   "source": [
    "Adding all the verses of Kendrick Lamar in a single file, without any manipulation. The number 1000 is arbitrary, I took into account the approximate number of songs on his Genius page, and will trim this number in subsequent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e7c0c-5631-467c-8831-bb63dece7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius('2HAd9TpvecWhy-9tvpHsAhIbQvbHVTcvL4jq7TkKCcVhGrb9H-agDcTxYEhTP0vE', timeout=9999)\n",
    "artist = genius.search_artist(\"Kendrick Lamar\", max_songs=1000, include_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5bbc0-cd0a-4f39-a989-c882720f4385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f283ac-0b2e-4360-ab12-9ce2ff87610a",
   "metadata": {},
   "source": [
    "I will remove a number of songs based on the lowest pageview number of a song that is officially released, not a leak, a demo, a mix or a live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fee08-8a19-4f09-b635-3c59473fed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Lyrics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "deleted_indexes = [181, 182, 201, 210, 211, 212, 216, 217, 220, 245, 249, 250, 257, 263, 273, \n",
    "                   276, 278, 281, 284, 286, 287, 289, 295, 296, 298, 299, 300, 302, 306, 319, \n",
    "                   322, 328, 329, 335, 336, 338, 345, 346, 351, 354, 364, 370, 382, 387, 411, \n",
    "                   413, 416, 417, 420, 429, 436, 437, 438, 440, 446, 451, 454, 455, 456, 458, \n",
    "                   461, 484, 485, 486, 489, 495, 498, 500, 517, 520, 525, 529, 533, 534, 535, \n",
    "                   538, 543, 544, 545, 546, 548, 549, 550, 552, 555, 560, 561, 563, 569, 572, \n",
    "                   573, 574, 575, 580, 587, 588, 591, 598, 600, 602, 603, 604, 606, 608, 611, \n",
    "                   612, 615, 617, 621, 622, 624, 626, 627, 628, 629, 630, 633, 635, 638, 643, \n",
    "                   645, 647, 648, 650, 651, 652, 654, 655, 656, 659, 660, 663, 664, 665, 667, \n",
    "                   668, 671, 672, 674, 677, 679, 682, 685, 686, 691, 692, 693, 694, 695, 696, \n",
    "                   697, 701, 702, 703, 705, 709, 710, 711, 714, 716, 717, 719, 721, 726, 729, \n",
    "                   731, 733, 735, 737, 738, 739, 740, 741, 743, 745, 746, 747, 748, 749, 751, \n",
    "                   753, 755, 758, 759, 762, 763, 764, 765, 766, 767, 768, 769, 771, 772, 773, \n",
    "                   775, 776, 778, 779, 781, 786, 788, 789, 790, 792, 793, 794, 795, 796, 797, \n",
    "                   798, 800, 801, 804, 805, 806, 808, 809, 810, 812, 813, 814, 815, 816, 817, \n",
    "                   818, 819, 820, 822, 823, 826, 827, 828, 830, 831, 832, 833, 834, 835, 836, \n",
    "                   837, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 853, 854, 855, \n",
    "                   856, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 871, 872, 873, \n",
    "                   875, 876, 877, 878, 879, 880, 881, 882, 883, 885, 886, 887, 888, 890, 891, \n",
    "                   893, 895, 896, 898, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, \n",
    "                   911, 912, 913, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, \n",
    "                   927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, \n",
    "                   942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, \n",
    "                   957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, \n",
    "                   972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, \n",
    "                   987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n",
    "data[\"songs\"] = [song for i, song in enumerate(data[\"songs\"]) if i not in deleted_indexes]\n",
    "\n",
    "with open(\"Dataset/Lyrics_DeletedSongs1000_28.05.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61286da3-c696-4533-9e23-a31795094bdc",
   "metadata": {},
   "source": [
    "I will add manually 16 songs where Kendrick appears and he is credited as an artist, but the Lyricsgenius extract did not find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a5691-eac1-4998-aaa9-393557c6ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dataset/Lyrics_DeletedSongs500_28.05.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "   data = json.load(f)\n",
    "\n",
    "with open(\"songs_to_add_library.txt\", \"r\", encoding=\"utf-8\") as g:\n",
    "    songs = [song.strip() for song in g]\n",
    "\n",
    "for i in range(0, len(songs)):\n",
    "    song = artist.song(songs[i])\n",
    "    data['songs'].append(song.to_dict())\n",
    "    \n",
    "with open(\"Dataset/Lyrics_AddedSongs16_29.05.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b80afe-7395-40e1-9efa-d0111de3a040",
   "metadata": {},
   "source": [
    "I will add 4 songs in which Kendrick appears with feature, but he is not credited as an artist and Lyricsgenius attributes the song to another artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61078921-97b3-4928-bbbe-7227946490c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by Vince Staples...\n",
      "\n",
      "Song 1: \"Norf Norf\"\n",
      "\n",
      "Reached user-specified song limit (1).\n",
      "Done. Found 1 songs.\n",
      "Searching for \"Opps\" by Vince Staples...\n",
      "Done.\n",
      "Searching for songs by Black Hippy...\n",
      "\n",
      "Song 1: \"U.O.E.N.O.\"\n",
      "\n",
      "Reached user-specified song limit (1).\n",
      "Done. Found 1 songs.\n",
      "Searching for \"On Some Other Shit\" by Black Hippy...\n",
      "Done.\n",
      "Searching for \"Scenario Freestyle\" by Black Hippy...\n",
      "Done.\n",
      "Searching for \"Try Me\" by Black Hippy...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "with open(\"Dataset/Lyrics_AddedSongs16_29.05.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "   data = json.load(f)\n",
    "\n",
    "artist = genius.search_artist(\"Vince Staples\", max_songs=1)\n",
    "song = artist.song(\"Opps\")\n",
    "data['songs'].append(song.to_dict())\n",
    "\n",
    "artist = genius.search_artist(\"Black Hippy\", max_songs=1)\n",
    "song = artist.song(\"On Some Other Shit\")\n",
    "data['songs'].append(song.to_dict())\n",
    "song = artist.song(\"Scenario Freestyle\")\n",
    "data['songs'].append(song.to_dict())\n",
    "song = artist.song(\"Try Me\")\n",
    "data['songs'].append(song.to_dict())\n",
    "\n",
    "with open(\"Dataset/Lyrics_AddedSongsManually4_29.05.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9668f55-7d84-422d-a4ca-725483afb25b",
   "metadata": {},
   "source": [
    "Saving only \n",
    "-title\n",
    "-id\n",
    "-pageviews(future update, for relevance in lyrics)\n",
    "-description(which helps with removing some unnecessary words from lyrics)\n",
    "-lyrics\n",
    "-album(future update, make a song in the style of this album)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0539cd7d-9611-4b12-8e8a-fe4452690614",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dataset/Lyrics_AddedSongsManually4_29.05.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "filtered_songs = []\n",
    "\n",
    "for song in data.get(\"songs\", []):\n",
    "    if song.get(\"album\") is not None:\n",
    "        new_song = {\n",
    "            \"title\": song.get(\"title\"),\n",
    "            \"id\": song.get(\"id\"),\n",
    "            \"pageviews\": song.get(\"stats\", {}).get(\"pageviews\"),\n",
    "            \"description\": song.get(\"description\"),\n",
    "            \"lyrics\": song.get(\"lyrics\"),\n",
    "            \"album\": song.get(\"album\", {}).get(\"name\")\n",
    "        }\n",
    "    else:\n",
    "            new_song = {\n",
    "            \"title\": song.get(\"title\"),\n",
    "            \"id\": song.get(\"id\"),\n",
    "            \"pageviews\": song.get(\"stats\", {}).get(\"pageviews\"),\n",
    "            \"description\": song.get(\"description\"),\n",
    "            \"lyrics\": song.get(\"lyrics\"),\n",
    "            \"album\": song.get(\"album\")\n",
    "    }\n",
    "    filtered_songs.append(new_song)\n",
    "\n",
    "new_data = {\"songs\": filtered_songs}\n",
    "\n",
    "with open(\"Dataset/Lyrics_FilteredFields_29.05.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502b2d1-b253-46fe-a765-a03fedb29641",
   "metadata": {},
   "source": [
    "Clean the lyrics of unnecesary words (description is doubled in lyrics + data that is irrelevant to lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b8e7852f-57ab-45cf-bd46-87e09016853f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"Dataset/Lyrics_FilteredFields_29.05.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for song in data.get(\"songs\", []):\n",
    "    lyrics = song.get(\"lyrics\")\n",
    "    clean_lyric = lyrics.split(\"Lyrics\", 1)\n",
    "    if len(clean_lyric) > 1:\n",
    "        song[\"lyrics\"] = clean_lyric[1]\n",
    "\n",
    "    lyrics = song.get(\"lyrics\")\n",
    "    description = song.get(\"description\", {}).get(\"plain\")\n",
    "\n",
    "    if \"Read More\" in lyrics:\n",
    "        song[\"lyrics\"] = lyrics.split(\"Read More\", 1)[1].lstrip()\n",
    "    \n",
    "    if lyrics.startswith(description):\n",
    "        song[\"lyrics\"] = lyrics[len(description):].lstrip(\"\\n\")\n",
    "\n",
    "with open(\"Dataset/Lyrics_ExtractJustLyrics_29.05.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bd6a6-5f1f-48c0-9619-67756dc1e9e1",
   "metadata": {},
   "source": [
    "A function to extract only Kendrick Lyrics from songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "31cf3233-c99c-4344-ad8e-8d12130c2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_kendrick_and_unlabeled_lyrics(text):\n",
    "    output = []\n",
    "    current_speaker = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        header_match = re.match(r\"\\[(.*?)\\]\", line.strip())\n",
    "        if header_match:\n",
    "            if buffer and current_speaker:\n",
    "                output.extend(buffer)\n",
    "            buffer = []\n",
    "\n",
    "            tag_content = header_match.group(1).strip().lower()\n",
    "            \n",
    "            if \":\" not in tag_content:\n",
    "                current_speaker = \"kendrick_or_unlabeled\"\n",
    "            elif \"kendrick lamar\" in tag_content:\n",
    "                current_speaker = \"kendrick_or_unlabeled\"\n",
    "            else:\n",
    "                current_speaker = None\n",
    "        else:\n",
    "            if current_speaker == \"kendrick_or_unlabeled\":\n",
    "                buffer.append(line.strip())\n",
    "\n",
    "    if buffer and current_speaker == \"kendrick_or_unlabeled\":\n",
    "        output.extend(buffer)\n",
    "\n",
    "    return \"\\n\".join(line for line in output if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "938b23ea-f456-48f1-9b76-8d31c3531696",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dataset/Lyrics_ExtractJustLyrics_29.05.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for song in data.get(\"songs\", []):\n",
    "    lyrics = song.get(\"lyrics\", \"\")\n",
    "    song[\"lyrics\"] = extract_kendrick_and_unlabeled_lyrics(lyrics)\n",
    "     \n",
    "data = [songs for songs in data.get(\"songs\", []) if songs[\"lyrics\"] != \"\"]\n",
    "\n",
    "with open(\"Dataset/Lyrics_ExtractKLLyrics_29.05.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba972d8-4688-4c57-aacd-745fd320fee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires dill<0.3.9,>=0.3.0, which is not installed.\n",
      "datasets 3.6.0 requires filelock, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, which is not installed.\n",
      "datasets 3.6.0 requires pandas, which is not installed.\n",
      "datasets 3.6.0 requires pyarrow>=15.0.0, which is not installed.\n",
      "huggingface-hub 0.32.3 requires filelock, which is not installed.\n",
      "huggingface-hub 0.32.3 requires fsspec>=2023.5.0, which is not installed.\n",
      "spacy 3.8.7 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, which is not installed.\n",
      "spacy 3.8.7 requires typer<1.0.0,>=0.3.0, which is not installed.\n",
      "transformers 4.52.4 requires filelock, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7cb7ef-5064-4ae5-aca2-3621b7fd3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df3ef5-bb17-4098-9461-5e9a3be649a7",
   "metadata": {},
   "source": [
    "Preparing the dataset for training and creating the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49d9efc-b150-462a-ba06-e0fad1f6f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dataset/Lyrics_ExtractTopics_12.06.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"Training/training_data.txt\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for song in data:\n",
    "        theme = song[\"theme\"].lower()\n",
    "        lyrics = song.get(\"lyrics\",\"\")\n",
    "        out_f.write(f\"<|theme|> {theme}\\n<|lyrics|> {lyrics}\\n<|endoftext|>\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146b3c0c-94a2-4c19-93b3-b46e3bf66029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\alex\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alex\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\alex\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\alex\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex\\.conda\\envs\\disertatie-primary\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62d3568-9d11-4165-b5d5-9f6f653e8f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\.conda\\envs\\disertatie-primary\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 32093 examples [00:00, 1055672.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import logging\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"Training/training_data.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fab4c72-9f27-4f99-803b-82b7e445c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 32093\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920b154b-bfb6-4544-a2fb-430237be422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████| 32093/32093 [00:10<00:00, 2983.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(data):\n",
    "    return tokenizer(data[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00681185-5b8a-47b9-b611-aecedaee53a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|████████████████████████████████| 32093/32093 [00:00<00:00, 363567.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"Training/tokenized_kendrick\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-disertatie-primary]",
   "language": "python",
   "name": "conda-env-.conda-disertatie-primary-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
